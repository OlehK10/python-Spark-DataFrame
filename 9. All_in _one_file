from datetime import datetime, date
from functools import wraps
from pyspark.sql import DataFrame
import pyspark
from pyspark.sql import SparkSession
import pyspark.sql.functions as F
from pyspark.sql.functions import udf
from pyspark.sql.functions import col
from pyspark.sql.types import IntegerType, StructType, StructField, StringType, DateType, FloatType

class Utils():
    """class вспомогательных функций"""
    def find_missing(lst:list) -> list:
        """

        :return:
        """
        freq_dict = {i: 0 for i in range(min(lst), max(lst) + 2)}

        for i in lst:
            freq_dict[i] += 1

        return min([key for key, val in freq_dict.items() if val == 0])

    def sum1(x, year):
        if x is None or year is None: return float(0)
        elif 0 <= year < 1:
            return float(x)
        elif 1 <= year < 2:
            return float(x + 10)
        elif 2 <= year < 3:
            return float(x * 1.2)
        elif 3 <= year < 5:
            return float(x * 1.3)
        elif year > 5:
            return float(x * 1.5)

    sum_cols = udf(sum1, FloatType())

class AdsVentureCount():
    app_name = 'Grand'
    # ----Table 1
    data1 = [("1", "Kharkov Filial", "Kharkov"),
             ("2", "Kiev Filial", "Kiev",),
             ("3", "Dnepr Filial", "Dnepr",),
             ("4", "Lviv Filial", "Lviv"),
             ("5", "New Filial", "Lviv")]
    columns1 = ["DEPTNO", "DNAME", "LOC"]
    #----Table 2
    data2 = [("1", "Ivanov", "head filial", None, date(1995,12,16), 100000, 1),
             ("2", "Petrov", "meneger", "1", date(2000,11,16), 50000, 1),
             ("3", "Sidorov", "developer", "1", date(2010,9,16), 50000, 1),
             ("4", "Sidorov2", "developer", "1", date(2011,12,16), 45000, 1),
             ("5", "Sidorov3", "head filial", None, date(2005,12,16), 145000, 2),
             ("7", "Sidorov4", "meneger", "5", date(2015,10,16), 50000, 2),
             ("8", "Sidorov5", "developer", "5", date(1992,3,16), 85000, 2),
             ("9", "Sidorov6", "head filial", None, date(2020,12,16), 95000, 3),
             ("10", "Sidorov7", "meneger", "9", date(2021,12,16), 135000, 3),
             ("11", "Sidorov8", "head filial", "1", date(2022,6,16), 75000, 4)]
    # columns2 = StructType(["EMPNO", "ENAME", "JOB", "MGR", "HIREDATE", "SAL", "DEPTNO"])
    columns2 = StructType( \
        [\
            StructField("EMPNO", StringType(),True), \
            StructField("ENAME", StringType(),True), \
            StructField("JOB", StringType(),True), \
            StructField("MGR", StringType(),True), \
            StructField("HIREDATE", DateType(),True), \
            StructField("SAL", IntegerType(), True), \
            StructField("DEPTNO", IntegerType(),True) \
        ])
    def ct_time(func):
        @wraps(func)
        def inner(*args, **kwargs):
            start = datetime.now()
            tmp = func(*args, **kwargs)
            ct_tm = datetime.now() - start
            print("count_time: ", ct_tm)
            return tmp

        return inner

    @ct_time
    def run(self):

        spark = SparkSession.builder.appName(self.app_name).getOrCreate()
        DEPT = spark.createDataFrame(self.data1, self.columns1)
        EMP = spark.createDataFrame(self.data2, self.columns2)
        DEPT.show()
        EMP.show()
        # ---- cheng_colims
        print('DEPT_')
        DEPT = AdsVentureCount.change_name_columns(DEPT, 'DEPT_')
        EMP = AdsVentureCount.change_name_columns(EMP, 'EMP_')
        print('DEPT_')
        DEPT.show()
        print('EMP_')
        EMP.show()
        # ----join 2 Table
        source_df = AdsVentureCount.join_table(DEPT, EMP, DEPT['DEPT_DEPTNO'] == EMP['EMP_DEPTNO'])
        source_df.show()
        # ----1
        print("# ----1")
        first_problem = AdsVentureCount.count_EMP(source_df)
        first_problem.show()
        print("# ----2")
        two_problem = AdsVentureCount.head_DEPT(spark, source_df)
        two_problem.show()
        # ----3
        print("# ----3")
        three_problem = AdsVentureCount.min_EMPNO(EMP)
        print("min_EMPNO : ",three_problem)
        print("# ----4")
        max_min_EMP = AdsVentureCount.max_min_EMPNO(spark, source_df)
        max_min_EMP.show()
        # ----5
        print("# ----5")
        sal_value = AdsVentureCount.sal_value_mod(source_df)
        sal_value.show()

    @staticmethod
    def change_name_columns(VAR_DF: DataFrame, PREFIX_: str) -> DataFrame:
        """
        :param VAR_DF: :DataFrame
        :param PREFIX_: : string <- the word to be added before the given column name
        -------
        :returns : DataFrame

        -------
        -------
        Examples

        VAR_DF : DataFrame

        PREFIX_: DEPT_ <- the word to be added before the given column name

        input:
            ============ ==========
            DEPTNO          EMP
            ============ ==========
                      1        4
                      2        3
            ============ ==========

        out : DataFrame
            ============ ==========
            DEPT_DEPTNO  DEPT_EMP
            ============ ==========
                      1        4
                      2        3
            ============ ==========
        """

        for column in VAR_DF.columns:
            # print('column = ', column)
            new_column = column.replace(column, PREFIX_ + column)
            VAR_DF = VAR_DF.withColumnRenamed(column, new_column)

        return VAR_DF

    @staticmethod
    def join_table(DEPT: DataFrame, EMP: DataFrame, joinExpression) -> DataFrame:

        df_new = DEPT.join(EMP, joinExpression, 'leftouter')

        return df_new

    @staticmethod
    def count_EMP(df: DataFrame) -> DataFrame:
        output = (
            df
            .groupBy('DEPT_DEPTNO')
            .agg(
                F.count('EMP_DEPTNO').alias('count_EMP')
            )
        )
        return output.sort(F.desc('count_EMP'))

    @staticmethod
    def head_DEPT(spark_session: SparkSession, df: DataFrame) -> DataFrame:
        df.createOrReplaceTempView("ALL_EMP")
        sqlDF = spark_session.sql("SELECT "
                                  "EMP_EMPNO, EMP_ENAME, EMP_JOB, EMP_MGR, EMP_HIREDATE, EMP_SAL, EMP_DEPTNO "
                                  "FROM ALL_EMP where "
                                  "EXISTS "
                                  "(SELECT 1 FROM ALL_EMP ALL_EMP1 where ALL_EMP.EMP_MGR  = ALL_EMP1.EMP_EMPNO and ALL_EMP.EMP_DEPTNO != ALL_EMP1.EMP_DEPTNO) "
                                  "UNION "
                                  "SELECT "
                                  "EMP_EMPNO, EMP_ENAME, EMP_JOB, EMP_MGR, EMP_HIREDATE, EMP_SAL, EMP_DEPTNO "
                                  "FROM ALL_EMP where "
                                  "ALL_EMP.EMP_MGR is NULL and ALL_EMP.EMP_EMPNO is not NULL"
                                  )

        return sqlDF

    @staticmethod
    def min_EMPNO(df: DataFrame) -> list:
        states1 = df.rdd.map(lambda x: x.EMP_EMPNO).collect()
        lst = [int(x) for x in states1]
        return Utils.find_missing(lst)

    @staticmethod
    def max_min_EMPNO(spark_session: SparkSession, df: DataFrame) -> DataFrame:
        df.createOrReplaceTempView("max_min_EMPNO")
        sqlDF = spark_session.sql("SELECT * from ( "
                                  "SELECT "
                                  "DEPT_DEPTNO, DEPT_DNAME, EMP_ENAME, EMP_SAL as SAL_MAX,"
                                  "dense_rank() over (partition by DEPT_DEPTNO order by EMP_SAL desc) as myrank "
                                  "FROM max_min_EMPNO "
                                  "where "
                                  "max_min_EMPNO.EMP_EMPNO is not NULL "
                                  ") emp_max, "
                                  "( "
                                  "SELECT "
                                  "DEPT_DEPTNO as DEPT_DEPTNO_MIN, DEPT_DNAME as DNAME_MIN, EMP_ENAME as ENAME_MIN, EMP_SAL AS SAL_MIN,"
                                  "dense_rank() over (partition by DEPT_DEPTNO order by EMP_SAL) as myrank_min "
                                  "FROM max_min_EMPNO "
                                  "where "
                                  "max_min_EMPNO.EMP_EMPNO is not NULL "
                                  ") emp_min "
                                  "where "
                                  "emp_max.DEPT_DEPTNO = emp_min.DEPT_DEPTNO_MIN "
                                  "and emp_max.myrank = 1 "
                                  "and emp_min.myrank_min = 1 "
                                  )

        return sqlDF

    @staticmethod
    def sal_value_mod(df: DataFrame) -> DataFrame:
        # increase_salaryUDF = udf(lambda x, y: increase_salary(x, y), IntegerType())
        df = df.withColumn(
            'JOB_M',
            F.upper(col('EMP_JOB'))).\
            withColumn('Work_time(year)', F.round(F.months_between(F.current_date(), col("EMP_HIREDATE")) / F.lit(12), 2)) \
            .withColumn('SAL_increase',
                    Utils.sum_cols(df['EMP_SAL'].cast(IntegerType()),
                                    F.round(F.months_between(F.current_date(), col("EMP_HIREDATE")) / F.lit(12), 2)
                                )
                    )

        return df


if __name__ == '__main__':
    job = AdsVentureCount()
    job.run()
